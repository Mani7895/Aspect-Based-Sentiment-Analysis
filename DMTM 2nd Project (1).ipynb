{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from preprocessing import *\n",
    "from plotting import *\n",
    "import sys\n",
    "import seaborn\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import string\n",
    "import nltk\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pandas import read_table\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"C:/Users/Manika/Desktop/Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Manika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Manika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Manika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Manika\\\\Desktop\\\\Dataset'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import wordnet\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(url):\n",
    "    frame = read_table(url, encoding = \"utf-8\", sep = \",\", skipinitialspace = True, index_col = None, header = 0)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_out_text(text_string):\n",
    "    text_string = text_string.replace(\"[comma]\", \" \")\n",
    "    text_string = text_string.replace(\"\\t\", \" \")\n",
    "    text_string = text_string.replace(\"  \", \" \")\n",
    "    text_string = text_string.translate(''.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(clf, clf_name, X_train, X_test, y_train, y_test, with_ovr = False):\n",
    "    clf.fit(X_train, y_train)\n",
    "    try:\n",
    "        print(\"{}\".format(clf.best_estimator_))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    score = f1_score(y_test, pred, average='weighted')\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average=None)\n",
    "    recall = recall_score(y_test,pred,average=None)\n",
    "    print('{} (F1 score={:.3f}, Accuracy={:.4f})'.format(clf_name, score, acc))\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \",recall)\n",
    "                                \n",
    "    try:\n",
    "        y_prob = clf.decision_function(X_test)\n",
    "    except AttributeError:\n",
    "        # Handle BernoilliNB\n",
    "        y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    precision, recall, avg = get_per_class_pr_re_and_avg(y_test, y_prob) if with_ovr else (0.,0.,0.)\n",
    "    \n",
    "    \n",
    "    if with_ovr:\n",
    "        print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(avg[\"micro\"]))\n",
    "    return precision, recall, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tagged = nltk.pos_tag(tokens)\n",
    "    return \" \".join([lmtzr.lemmatize(w[0], pos = get_wordnet_pos(w[1])) for w in pos_tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_using_cv(X, y, use_ovr=False):\n",
    "    '''\n",
    "    Stand-alone function to executes and plot evaluation metrics using explicit (averaged) 10-Fold CV and SVM.\n",
    "    'use_ovr' for multiclass data.\n",
    "    Ref: http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_digits.html#sphx-glr-auto-examples-exercises-plot-cv-digits-py\n",
    "    '''\n",
    "    print(__doc__)\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn import datasets, svm\n",
    "\n",
    "    #digits = datasets.load_digits()\n",
    "    #X = digits.data\n",
    "    #y = digits.target\n",
    "    svc = svm.SVC(kernel='linear')\n",
    "\n",
    "    if use_ovr:\n",
    "        from sklearn.multiclass import OneVsRestClassifier\n",
    "        svc = OneVsRestClassifier(svc)\n",
    "\n",
    "    C_s = np.logspace(-2, 1, 10)\n",
    "\n",
    "    scores = list()\n",
    "    scores_std = list()\n",
    "    print(\"Running 10-Fold CV on Linear SVM, with varying C..\")\n",
    "    for C in C_s:\n",
    "        svc.C = C\n",
    "        this_scores = cross_val_score(svc, X, y, cv=10, n_jobs=4)\n",
    "        mean_score = np.mean(this_scores)\n",
    "        scores.append(mean_score)\n",
    "        scores_std.append(np.std(this_scores))\n",
    "        print(\"Score with C={} is {}\".format(C, mean_score))\n",
    "\n",
    "    # Do the plotting\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(1, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    plt.semilogx(C_s, scores)\n",
    "    plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\n",
    "    plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\n",
    "    locs, labels = plt.yticks()\n",
    "    plt.yticks(locs, list(map(lambda x: \"%g\" % x, locs)))\n",
    "    plt.ylabel('CV score')\n",
    "    plt.xlabel('Parameter C')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(text,aspect):\n",
    "    for word in aspect.split():\n",
    "     #print word,text\n",
    "     if text==word:\n",
    "        #print(\"True\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listsum(numList):\n",
    "    theSum = 0\n",
    "    for i in numList:\n",
    "        theSum = theSum + float(i)\n",
    "    return theSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lex_score(word):\n",
    "    #word='horrible'\n",
    "    lex_scores=[]\n",
    "    #########Laptop\n",
    "    if(lex_file_laptop.get(word)!=None):\n",
    "        lex_scores.append(lex_file_laptop.get(word))\n",
    "    \n",
    "    #########Res\n",
    "    if(lex_file_res.get(word)!=None):\n",
    "        lex_scores.append(lex_file_res.get(word)-0.5)\n",
    "        \n",
    "    ##########Bing LIU\n",
    "    if(lex_file_bing_negative.get(word)!=None):\n",
    "        lex_scores.append(lex_file_bing_negative.get(word))\n",
    "    if(lex_file_bing_positive.get(word)!=None):\n",
    "        lex_scores.append((lex_file_bing_positive.get(word)))\n",
    "    ###\n",
    "    if len(lex_scores)==0:\n",
    "        lex=0\n",
    "    else:\n",
    "        \n",
    "        lex=listsum(lex_scores)/len(lex_scores)\n",
    "        \n",
    "    \n",
    "    return lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(i,start_loc,end_loc):\n",
    "    #i=4\n",
    "    dep_par1=dep_par[i]\n",
    "    aspect_term1=aspect_term[i]\n",
    "    #start_loc=int(start_loc)-2\n",
    "    #end_loc=int(end_loc)+2\n",
    "    #dep_par1=dep_par[12]\n",
    "    #aspect_term1=aspect_term[12]\n",
    "    start_loc=int(loc_start[i])-2\n",
    "    end_loc=int(loc_end[i])+2\n",
    "    adj=[]\n",
    "    temp=[]\n",
    "    temp1=[]\n",
    "    temp2=[]\n",
    "    temp3=[]\n",
    "    temp4=[]\n",
    "    \n",
    "    for possible_subject in dep_par1:\n",
    "        temp.append(possible_subject)\n",
    "        if possible_subject.dep == advmod:\n",
    "            temp1.append(possible_subject.text)\n",
    "            if possible_subject.head.pos==VERB or possible_subject.head.pos==NOUN:\n",
    "                temp2.append(possible_subject.head.text)\n",
    "                if check_similarity(possible_subject.head.text,aspect_term1) and possible_subject.head.idx>=start_loc and possible_subject.head.idx<=end_loc:\n",
    "                    temp3.append(True)\n",
    "                    adj.append(possible_subject.text)\n",
    "                    \n",
    "    for possible_subject in dep_par1:\n",
    "        temp.append(possible_subject.head.text)\n",
    "        if possible_subject.pos == NOUN and check_similarity(str(possible_subject.head.text),aspect_term1) and possible_subject.dep_ == 'compound':\n",
    "          temp1.append(possible_subject.text)\n",
    "          temp3.append(check_similarity(str(possible_subject.text),aspect_term1))\n",
    "          if check_similarity(str(possible_subject.text),aspect_term1):\n",
    "                 for children in possible_subject.head.rights:\n",
    "                     temp2.append(children.text)\n",
    "                     if children.pos==NOUN:\n",
    "                         adj.append(children.text)\n",
    "    \n",
    "    for possible_subject in dep_par1:\n",
    "       if check_similarity(str(possible_subject.text),aspect_term1) and possible_subject.dep == dobj:\n",
    "           adj.append(possible_subject.head.text)\n",
    "           for children in possible_subject.head.lefts:\n",
    "               if children.pos==ADV:\n",
    "                adj.append(children.text)\n",
    "           \n",
    "                          \n",
    "    count=0\n",
    "    for possible_subject in dep_par1:\n",
    "        if possible_subject.dep == nsubj and check_similarity(str(possible_subject.text),aspect_term1):\n",
    "             temp.append(possible_subject.text)\n",
    "             for right_children in possible_subject.head.rights:\n",
    "                    temp2.append(right_children.text)\n",
    "                    for right_children1 in right_children.lefts:\n",
    "                        \n",
    "                         temp3.append(right_children1.text)\n",
    "                        \n",
    "                         if right_children1.dep==amod: \n",
    "                          adj.append(right_children1.text)\n",
    "                        \n",
    "                    \n",
    "    \n",
    "    for possible_subject in dep_par1:\n",
    "        if possible_subject.pos==NOUN :\n",
    "            count=count+1\n",
    "    if count==1:\n",
    "        for possible_subject in dep_par1:\n",
    "            if possible_subject.pos==ADV or possible_subject.pos==ADJ:\n",
    "                adj.append(possible_subject.text)\n",
    "        \n",
    "    for possible_subject in dep_par1:\n",
    "        if possible_subject.dep == nsubj and check_similarity(str(possible_subject.text),aspect_term1):\n",
    "                temp.append(possible_subject.text)\n",
    "            \n",
    "                for left_children in possible_subject.head.rights:\n",
    "                    temp2.append(left_children.text)\n",
    "                    if left_children.dep==attr:\n",
    "                        adj.append(left_children.text)\n",
    "                        \n",
    "    for possible_subject in dep_par1:\n",
    "        if possible_subject.dep == attr and check_similarity(str(possible_subject.text),aspect_term1):\n",
    "                temp.append(possible_subject.text)\n",
    "            \n",
    "                for left_children in possible_subject.head.lefts:\n",
    "                    temp2.append(left_children.text)\n",
    "                    if left_children.dep==nsubj:\n",
    "                        adj.append(left_children.text)\n",
    "    \n",
    "    for possible_subject in dep_par1:\n",
    "        if possible_subject.dep == pobj and check_similarity(str(possible_subject.text),aspect_term1):\n",
    "            temp.append(possible_subject.text)\n",
    "            if possible_subject.head.dep == prep:\n",
    "                temp1.append(possible_subject.head.text)\n",
    "                count=0\n",
    "                for left_children in possible_subject.head.ancestors:\n",
    "                    count=count+1\n",
    "                    if count!=2:\n",
    "                        temp2.append(left_children.text)\n",
    "                        if left_children.pos==ADJ:\n",
    "                            adj.append(left_children.text)\n",
    "                            \n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "        \n",
    "    \n",
    "    for possible_subject in dep_par1:\n",
    "        temp.append(possible_subject.text)\n",
    "        adj_caught=-1\n",
    "        adj_caught_loc=-1\n",
    "        if possible_subject.dep == acomp or possible_subject.dep == ccomp:\n",
    "            #temp1.append(True)\n",
    "            if possible_subject.head.pos == VERB:\n",
    "                #temp2.append(True)\n",
    "                \n",
    "                #temp3.append(possible_subject.head.text)\n",
    "                for left_children in possible_subject.head.lefts:\n",
    "                    if check_similarity(str(left_children.text),aspect_term1) and left_children.idx>=start_loc and left_children.idx<=end_loc:\n",
    "                     adj_caught=possible_subject.text\n",
    "                     adj_caught_loc=possible_subject.idx\n",
    "                     \n",
    "                     temp4.append(adj_caught_loc)\n",
    "                     adj.append(possible_subject.text)\n",
    "            if adj_caught != -1:\n",
    "             for possible_subject1 in dep_par1:\n",
    "            \n",
    "              if possible_subject1.dep == advmod:\n",
    "               temp1.append(possible_subject.text)\n",
    "               temp2.append(possible_subject.head.text) \n",
    "            \n",
    "               if str(adj_caught)==str(possible_subject1.head.text) :#and possible_subject.head.idx==adj_caught_loc :\n",
    "                temp3.append(True)\n",
    "                adj.append(possible_subject1.text)\n",
    "            #temp4.append(possible_subject.text)\n",
    "                       \n",
    "                \n",
    "    \n",
    "    for possible_subject in dep_par1:\n",
    "        temp.append(possible_subject)\n",
    "        if possible_subject.dep_ == 'compound' :\n",
    "            temp1.append(True)\n",
    "            if possible_subject.head.pos==NOUN or possible_subject.head.pos==ADJ:\n",
    "                temp2.append(True)\n",
    "                if check_similarity(possible_subject.head.text,aspect_term1) and possible_subject.head.idx>=start_loc and possible_subject.head.idx<=end_loc:\n",
    "                    temp3.append(True)\n",
    "                    adj.append(possible_subject.text)\n",
    "                    \n",
    "    for possible_subject in dep_par1:\n",
    "        temp.append(possible_subject)\n",
    "        if possible_subject.dep == amod:\n",
    "            temp1.append(True)\n",
    "            if possible_subject.head.pos==NOUN or possible_subject.head.pos==ADJ:\n",
    "                temp2.append(True)\n",
    "                if check_similarity(possible_subject.head.text,aspect_term1) and possible_subject.head.idx>=start_loc and possible_subject.head.idx<=end_loc:\n",
    "                    temp3.append(True)\n",
    "                    adj.append(possible_subject.text)\n",
    "    \n",
    "   \n",
    "    for possible_adj in dep_par1:\n",
    "        \n",
    "        if possible_adj.pos == ADJ:\n",
    "            \n",
    "            for possible_subject in possible_adj.children:\n",
    "                #print possible_subject.text,possible_adj.children\n",
    "                if possible_subject.text == aspect_term1:                  \n",
    "                    adj.append(possible_adj)\n",
    "                    break\n",
    "    if len(adj)==0:\n",
    "        for possible_subject in dep_par1:\n",
    "            if check_similarity(possible_subject.text,aspect_term1):\n",
    "                 temp1.append(True)\n",
    "#                 for left_children in possible_subject.head.lefts:\n",
    "#                     temp2.append(left_children.text)\n",
    "#                     adj.append(left_children.text)\n",
    "                 for left_children in possible_subject.head.rights:\n",
    "                     temp2.append(left_children.text)\n",
    "                     adj.append(left_children.text)\n",
    "    if len(adj)==0:\n",
    "        for possible_subject in dep_par1:\n",
    "            if check_similarity(possible_subject.text,aspect_term1):\n",
    "                 temp1.append(True)\n",
    "                 for left_children in possible_subject.head.lefts:\n",
    "                     temp2.append(left_children.text)\n",
    "                     adj.append(left_children.text)\n",
    "#                 for left_children in possible_subject.head.rights:\n",
    "#                     temp2.append(left_children.text)\n",
    "#                     adj.append(left_children.text)\n",
    "    if len(adj)==0:\n",
    "       for possible_adj in dep_par1:\n",
    "        \n",
    "        if possible_adj.pos == ADJ or possible_adj.pos == ADV:\n",
    "                                         \n",
    "                    adj.append(possible_adj.text)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    return set(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features_and_labels(frame, classes = None, binarize = False, skip_label = False):\n",
    "    arr = np.array(frame)\n",
    "    corpus = []\n",
    "\n",
    "    for d in np.take(arr, [1,2], axis = 1):\n",
    "        parsed_text = parse_out_text(d[0])\n",
    "        corpus.append(lemmatize(parsed_text))\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    y = np.zeros(X.shape[0]) if skip_label else np.array(arr[:, -1], dtype = np.float)\n",
    "\n",
    "    if binarize:\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        y = label_binarize(y, classes)\n",
    "\n",
    "    return X, y, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from C:\\Users\\Manika\\Desktop\\Dataset\\data-2_train.csv\n",
      "Processing 3602 samples with 5 attributes\n"
     ]
    }
   ],
   "source": [
    "url = r'C:\\Users\\Manika\\Desktop\\Dataset\\data-2_train.csv'\n",
    "print(\"Downloading data from {}\".format(url))\n",
    "frame = download_data(url)\n",
    "print(\"Processing {} samples with {} attributes\".format(len(frame.index), len(frame.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, _ = transform_features_and_labels(frame, [-1,0,1])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Running 10-Fold CV on Linear SVM, with varying C..\n",
      "Score with C=0.01 is 0.6007807120638463\n",
      "Score with C=0.021544346900318832 is 0.6007807120638463\n",
      "Score with C=0.046415888336127774 is 0.6007807120638463\n",
      "Score with C=0.1 is 0.6007807120638463\n",
      "Score with C=0.21544346900318834 is 0.6202192657280647\n",
      "Score with C=0.46415888336127775 is 0.6643395329071227\n",
      "Score with C=1.0 is 0.6799199126866146\n",
      "Score with C=2.154434690031882 is 0.6590701978740476\n",
      "Score with C=4.6415888336127775 is 0.6385428863086391\n",
      "Score with C=10.0 is 0.6288151408871254\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d700ebefd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execute_using_cv(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classifiers.append([LinearSVC(), \"LinearSVC\"])\n",
    "classifiers.append([BernoulliNB(), \"BernoulliNB\"])\n",
    "classifiers.append([MultinomialNB(), \"MultiNB\"])\n",
    "classifiers.append([KNeighborsClassifier(n_neighbors=10), \"kNN\"])\n",
    "classifiers.append([AdaBoostClassifier(n_estimators=50, learning_rate=1.0, algorithm='SAMME.R'), \"AdaBoost\"])\n",
    "classifiers.append([RandomForestClassifier(n_estimators=100), \"Random forest\"])\n",
    "#classifiers.append([SVC(kernel='rbf', class_weight='balanced', decision_function_shape='ovo'), \"Baseline ovo rbf SVM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC (F1 score=0.717, Accuracy=0.7282)\n",
      "Precision:  [0.68965517 0.47474747 0.79245283]\n",
      "Recall:  [0.54945055 0.39830508 0.89786223]\n",
      "BernoulliNB (F1 score=0.643, Accuracy=0.6657)\n",
      "Precision:  [0.61111111 0.37647059 0.72745098]\n",
      "Recall:  [0.42307692 0.27118644 0.88123515]\n",
      "MultiNB (F1 score=0.558, Accuracy=0.6436)\n",
      "Precision:  [0.66071429 0.58823529 0.64351852]\n",
      "Recall:  [0.2032967  0.08474576 0.99049881]\n",
      "kNN (F1 score=0.645, Accuracy=0.6713)\n",
      "Precision:  [0.6031746  0.41333333 0.725     ]\n",
      "Recall:  [0.41758242 0.26271186 0.89548694]\n",
      "AdaBoost (F1 score=0.604, Accuracy=0.6380)\n",
      "Precision:  [0.54098361 0.42372881 0.68333333]\n",
      "Recall:  [0.36263736 0.21186441 0.87648456]\n",
      "Random forest (F1 score=0.664, Accuracy=0.6852)\n",
      "Precision:  [0.65833333 0.41573034 0.73828125]\n",
      "Recall:  [0.43406593 0.31355932 0.89786223]\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': np.logspace(-2, 1, 10),\n",
    "                  #'kernel': ['rbf', 'linear', 'poly'],\n",
    "                  #'gamma': [0.0005, 0.001, 0.005, 0.01, 0.1, 1.0],     # Use for testing rbf, poly\n",
    "                 'degree': [1,2,3]}\n",
    "svm = GridSearchCV(SVC(kernel='linear', class_weight='balanced'), param_grid, cv=5)\n",
    "results = []\n",
    "\n",
    "for clf in classifiers:\n",
    "    results.append(execute(clf[0], clf[1], X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading test data from C:\\Users\\Manika\\Desktop\\Dataset\\data-2_train.csv\n"
     ]
    }
   ],
   "source": [
    "url = r'C:\\Users\\Manika\\Desktop\\Dataset\\data-2_train.csv'\n",
    "print(\"Downloading test data from {}\".format(url))\n",
    "frame_train = download_data(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading test data from C:\\Users\\Manika\\Desktop\\Dataset\\Data-1_test.csv\n"
     ]
    }
   ],
   "source": [
    "test_url = r'C:\\Users\\Manika\\Desktop\\Dataset\\Data-1_test.csv'\n",
    "#test_url = r'C:\\Users\\Manika\\Desktop\\Dataset\\Data-2_test.csv'\n",
    "print(\"Downloading test data from {}\".format(test_url))\n",
    "frame_test = download_data(test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3602 training samples with 5 attributes\n"
     ]
    }
   ],
   "source": [
    "# Process data into feature and label arrays\n",
    "print(\"Processing {} training samples with {} attributes\".format(len(frame_train.index), len(frame_train.columns)))\n",
    "X_train, y_train, fnames_trian = transform_features_and_labels(frame_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 638 test samples with 4 attributes\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing {} test samples with {} attributes\".format(len(frame_test.index), len(frame_test.columns)))\n",
    "X_test, y_test, fnames_test = transform_features_and_labels(frame_test, skip_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_keep_train = np.nonzero(np.in1d(fnames_trian, fnames_test))[0]\n",
    "i_keep_test = np.nonzero(np.in1d(fnames_test, fnames_trian))[0]\n",
    "X_train = X_train[:, i_keep_train]\n",
    "X_test = X_test[:, i_keep_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "score = f1_score(y_test, pred, average='weighted')\n",
    "acc = accuracy_score(y_test, pred)\n",
    "#print('{} (F1 score={:.3f}, Accuracy={:.4f})'.format(\"LinearSVC\", score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predictions to file..\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing predictions to file..\")\n",
    "import os\n",
    "result_file = \"Manika_Maheshwari_Rahul_Sathe.txt\";\n",
    "try:\n",
    "    os.remove(result_file)\n",
    "except OSError:\n",
    "    pass\n",
    "file = open(result_file, \"w\") \n",
    "frame_test_arr = np.array(frame_test)\n",
    "\n",
    "for i,p in enumerate(pred):\n",
    "    file.write(\"{};;{}\\n\".format(frame_test_arr[i][0], p))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Manika\\\\Desktop\\\\Dataset'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
